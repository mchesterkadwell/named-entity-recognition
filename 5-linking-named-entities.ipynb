{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linking Named Entities\n",
    "---\n",
    "---\n",
    "\n",
    "## Named Entities and Linked Data\n",
    "The named entities we have recognised in the Henslow data would be much more useful if they could be linked to other data known about those entities. This principle is called **linked data**. Linked data can enrich the discovery of collections and allow sophisticated searches for the knowledge within those collections.\n",
    "\n",
    "If the data is freely available and openly licensed it is known as **linked open data (LOD)**. The diagram below shows the extent of LOD in 2010. Since then then the **linked open data cloud** has grown immensely and you can explore it for yourself at [www.lod-cloud.net](https://www.lod-cloud.net/).\n",
    "\n",
    "<img src=\"https://lod-cloud.net/versions/2010-09-22/lod-cloud_colored.png\" alt=\"Linked open data cloud in 2010\" title=\"Linked open data cloud in 2010\">\n",
    "\n",
    "Linked data is a very big topic, so this notebook will only touch on a few introductory aspects that relate to the NER we have done in this course. In particular, we will focus on the automated ways of linking data that can be enabled by writing code, though the underlying principles can be understood without it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "## Disambiguate with an Authority File\n",
    "One of the challenges with named entities is that there may be many different forms, spellings or abbreviations that refer to the same person, place, country, and so on. \n",
    "\n",
    "An **authority file** is a way of normalising and unifying this information for each entity into a single, authoritative **authority record** and giving it a **unique identifier**. Typically, all the known forms of a particular entity will be recorded in its authority record so that every form can be resolved to the same, correct entity.\n",
    "\n",
    "You may already be familiar with [VIAF: The Virtual International Authority File](https://viaf.org/), which is an authority service that unifies multiple authority files for people, geographic places, works of art, and more.\n",
    "\n",
    "![VIAF: The Virtual International Authority File](http://www.bnc.cat/var/bnc_site/storage/images/el-blog-de-la-bc/viaf/1729168-1-cat-ES/VIAF_large.png)\n",
    "\n",
    "By simply [searching for a name in the search box](https://viaf.org/viaf/27063124/#Darwin,_Charles,_1809-1882.), it returns a VIAF ID, preferred and related names, and associated works. \n",
    "\n",
    "![assets/viaf-charles-darwin.png](assets/viaf-charles-darwin.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "## Lookup Entities Programmatically with Web APIs\n",
    "The power of centralised authorities such as VIAF is when their data is exposed via an **API** (Application Programming Interface). A web API is accessed via a particular web address and allows computer programs to request information and receive it in a structured format suitable for further processing. Typically, this data will be provided in either JSON or XML.\n",
    "\n",
    "VIAF has several different APIs. The one we will use is [Authority Cluster](https://www.oclc.org/developer/api/oclc-apis/viaf/authority-cluster.en.html) Auto Suggest. Sadly, OCLC have removed their OCLC API Explorer, which was really handy for exploring the API as a human! ðŸ˜ž\n",
    "\n",
    "In the old OCLC API Explorer we could search for \"john stevens henslow\" or any personal name from the Henslow letters:\n",
    "\n",
    "![assets/viaf-api-charles-darwin.png](assets/viaf-api-charles-darwin.png)\n",
    "\n",
    "It returned a list of results, in JSON format, with VIAF's suggestions for the best match, which you can see in the right-hand \"Response\" pane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can consume this data programmatically using Python tools with which we are already familiar from earlier notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Make the query to the API\n",
    "query = \"john stevens henslow\"\n",
    "response = requests.get('http://www.viaf.org/viaf/AutoSuggest?query=' + query)\n",
    "\n",
    "# Parse the JSON into a Python dictionary\n",
    "data = json.loads(response.text)\n",
    "\n",
    "# Get just the first entry in the results\n",
    "data['result'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you compare this with the output of the API explorer above, you should see this is the same structure and information.\n",
    "\n",
    "The VIAF ID is found in the `'vaifid'` field:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['result'][0]['viafid']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this information we could now enrich the original XML with the VIAF ID for this named entity.\n",
    "\n",
    "> **EXERCISE**: What are the problems you could anticipate with this sort of automated linking?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "## Lookup Named Entities in Bulk using Web APIs\n",
    "The scientific community has been busy normalising, disambiguating, and aggregating similar types of data for decades, in a movement parallel but largely separate to developments in library science and humanities. \n",
    "\n",
    "[The Global Biodiversity Information Facility (GBIF)](https://www.gbif.org/what-is-gbif) is an international open data aggregator for hundreds of millions of species records. \n",
    "\n",
    "![Global Biodiversity Information Facility](http://data.biodiversity.be/gbif-logo.png)\n",
    "\n",
    "In the [last notebook](4-updating-the-model-on-henslow-data.ipynb#Add-a-New-Entity-Type) we tried to add a new named entity type `TAXONOMY` for the model to learn. We defined this as a type of entity for any Linnaean taxonomic name (domain, kingdom, phylum, division, class, order, family, genus or species). Binomials (genus plus species together) were labelled as one span. \n",
    "\n",
    "Imagine if we wished to link these named taxonomic entities to the corresponding genus or species in the GBIF. How could we do this *en masse*?\n",
    "\n",
    "Like VIAF, [GBIF also has a set of web APIs](https://www.gbif.org/developer/summary) and we can use the [Species API](https://www.gbif.org/developer/species) to search for species names.\n",
    "\n",
    "> **EXERCISE**: Reading API documentation is a common activity for coders. Before you look at the code example below, open the [Species API](https://www.gbif.org/developer/species) documentation, scroll down to the 'Searching Names' section and see if you can work out which of the four resource URLs would be most useful for our case.\n",
    "\n",
    "<a title=\"Flowers of Pulmonaria officinalis. Pharaoh Hound at the English language Wikipedia, CC BY-SA 3.0 &lt;http://creativecommons.org/licenses/by-sa/3.0/&gt;, via Wikimedia Commons\" href=\"https://commons.wikimedia.org/wiki/File:Lungwort.jpg\"><img width=\"256\" alt=\"Lungwort: Flowers of Pulmonaria officinalis\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/7/74/Lungwort.jpg/256px-Lungwort.jpg\"></a>\n",
    "<p style=\"text-align: center; font-style: italic;\">Flowers of Pulmonaria officinalis</p>\n",
    "\n",
    "Let's start by trying one taxonomic (genus) name \"Pulmonaria\" to see what sort of result we can expect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Pulmonaria\"\n",
    "response = requests.get('https://api.gbif.org/v1/species/suggest?q=' + query)\n",
    "\n",
    "data = json.loads(response.text)\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, so very similar to the VIAF API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconciling Historical Taxa\n",
    "In reality, we need to be aware that some of the older names given to organisms in the Henslow letters are not easily reconciled with modern named taxa. (In the Darwin Correspondence Project (DCP), Shelley Innes, editor and research associate, is an expert in historical taxonomy and her work is available in the footnotes of the published DCP letters.)\n",
    "\n",
    "Also, the Henslow letters often use ligature ash ('Ã¦') rather than 'ae', which is used in family names in GBIF. The GBIF `suggest` API does not recognise 'Ã¦' and 'ae' as equivalent so either our queries will need to be normalised, or we can try a different API.\n",
    "\n",
    "<a title=\"Gall Wasp - Cynipidae family, Leesylvania State Park, Woodbridge, Virginia. Judy Gallagher, CC BY 2.0 &lt;https://creativecommons.org/licenses/by/2.0&gt;, via Wikimedia Commons\" href=\"https://commons.wikimedia.org/wiki/File:Gall_Wasp_-_Cynipidae_family,_Leesylvania_State_Park,_Woodbridge,_Virginia.jpg\"><img width=\"256\" alt=\"Gall Wasp - Cynipidae family, Leesylvania State Park, Woodbridge, Virginia\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/a/a6/Gall_Wasp_-_Cynipidae_family%2C_Leesylvania_State_Park%2C_Woodbridge%2C_Virginia.jpg/256px-Gall_Wasp_-_Cynipidae_family%2C_Leesylvania_State_Park%2C_Woodbridge%2C_Virginia.jpg\"></a>\n",
    "<p style=\"text-align: center; font-style: italic;\">Gall Wasp - Cynipidae family</p>\n",
    "\n",
    "If there is no matchable name in GBIF we get an empty result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"CynipidÃ¦\"\n",
    "response = requests.get('https://api.gbif.org/v1/species/suggest?q=' + query)\n",
    "\n",
    "data = json.loads(response.text)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But if we try the `search` API instead there is no problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"CynipidÃ¦\"\n",
    "response = requests.get('https://api.gbif.org/v1/species/search?q=' + query)\n",
    "\n",
    "data = json.loads(response.text)\n",
    "data['results'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now take the list of taxonomic names from the previous notebook, cleaned up and normalised, and try to make an query with the whole list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxonomy = [\n",
    " 'Adippe',\n",
    " 'Alisma repens',\n",
    " 'Alopecurus bulbosus',\n",
    " 'Althaea hirsuta',\n",
    " 'Anthemis Cotula',\n",
    " 'Anthericum serotinum',\n",
    " 'Anthyllis vulneraria',\n",
    " 'Apargia hirta',\n",
    " 'Arabis thaliana',\n",
    " 'Araucaria imbricata',\n",
    " 'Artemisia gallica',\n",
    " 'Asclepiadeae',\n",
    " 'Aspidia',\n",
    " 'Asterophyllites',\n",
    " 'Atriplex laciniata',\n",
    " 'Bechera grandis',\n",
    " 'Blysmus compressus',\n",
    " 'Bos',\n",
    " 'Bos primigenius',\n",
    " 'Campanula rapunculus',\n",
    " 'Campanula rotundifolia',\n",
    " 'Carex',\n",
    " 'Carex laevigata',\n",
    " 'Centaurea solstitialis',\n",
    " 'Cerastium humile',\n",
    " 'Chara gracilis',\n",
    " 'Cheiranthus sinuatus',\n",
    " 'Chiasognathus Grantii',\n",
    " 'Chironia littoralis',\n",
    " 'Chrysosplenium alternifolium',\n",
    " 'Cirisia',\n",
    " 'Cochlearia danica',\n",
    " 'Commelina coelestis',\n",
    " 'Corbula costata',\n",
    " 'Coryphodon',\n",
    " 'Cracidae',\n",
    " 'Crocus sativus',\n",
    " 'Cryllas',\n",
    " 'Cucubalus baccifer',\n",
    " 'Cuscuta Epilinum',\n",
    " 'Cycas',\n",
    " 'Cycas circinalis',\n",
    " 'Cyperus',\n",
    " 'Cytheraea obliqua',\n",
    " 'Daucus maritimum',\n",
    " 'Dianthus caryophyllus',\n",
    " 'Digitalis',\n",
    " 'Diptera',\n",
    " 'Epilobium hirsutm',\n",
    " 'Eriocaulon',\n",
    " 'Eriophorum',\n",
    " 'Eriophorum polystachion',\n",
    " 'Eriophorum pubescens',\n",
    " 'Euphorbia portlandica',\n",
    " 'Favularia nodosa',\n",
    " 'G.campestris',\n",
    " 'Gallinula Baillonii',\n",
    " 'Glaucium violaceum',\n",
    " 'Globulus',\n",
    " 'Hedysarum',\n",
    " 'Hedysarum scandens',\n",
    " 'Hemiptera',\n",
    " 'Holoptychus',\n",
    " 'Hortus Siccus',\n",
    " 'Hymenophyllum tunbridgense',\n",
    " 'Iberis amara',\n",
    " 'Inula',\n",
    " 'Inula helenium ',\n",
    " 'Jungermanniae',\n",
    " 'Knautia',\n",
    " 'Lathyrus hirsutus',\n",
    " 'Lepidoptera',\n",
    " 'Linosyris',\n",
    " 'Linum angustifol',\n",
    " 'Lobelia urens',\n",
    " 'Lonicera caprifolium',\n",
    " 'Lysimachia',\n",
    " 'Malaxis Loeslii',\n",
    " 'Medicago denticulata',\n",
    " 'Melampyrum arvense',\n",
    " 'Melissa',\n",
    " 'Mentha gentilis',\n",
    " 'Menyanthes Nymphaeoides',\n",
    " 'Mespilus cotoneaster',\n",
    " 'Milium lendigerum',\n",
    " 'Narcissus poeticus',\n",
    " 'Nemeolius Lucina',\n",
    " 'Neuropteris cordata',\n",
    " 'Oenanthe crocata',\n",
    " 'Ophrys arachnites',\n",
    " 'Orchideae',\n",
    " 'Orobanche caryophylacea',\n",
    " 'Panicum viride',\n",
    " 'Peperomia',\n",
    " 'Phleum paniculatum',\n",
    " 'Pisidium',\n",
    " 'Polyporites Bowmanni',\n",
    " 'Potamides plicatus',\n",
    " 'Potamogeton fluitans',\n",
    " 'Potamogeton gramineum',\n",
    " 'Pothos',\n",
    " 'Primula',\n",
    " 'Primula scotica',\n",
    " 'Primula vulgaris',\n",
    " 'Psammobia rudis',\n",
    " 'Pulicaria',\n",
    " 'Pyrola',\n",
    " 'Pyrola minor',\n",
    " 'Pyrus pinnatifida',\n",
    " 'Pyrus torminalis',\n",
    " 'Quercus sessiliflora',\n",
    " 'Ribes alpinum',\n",
    " 'Rubus idaeus',\n",
    " 'Ruppia',\n",
    " 'Ruppia maritima',\n",
    " 'Salicornia radicans',\n",
    " 'Salvia pratensis',\n",
    " 'Santolina maritima',\n",
    " 'Scirpus caricinus',\n",
    " 'Scirpus pauciflorus',\n",
    " 'Sisymbrium monense',\n",
    " 'Sonchus palustris',\n",
    " 'Statice cordata',\n",
    " 'Tetrandria',\n",
    " 'Thalassophytes',\n",
    " 'Tormentilla reptans',\n",
    " 'Trifolium',\n",
    " 'Trifolium subterraneum',\n",
    " 'Turritis hirsuta',\n",
    " 'Typha',\n",
    " 'Ulmus suberosa',\n",
    " 'Vaccinium myrtillus',\n",
    " 'Velleius',\n",
    " 'Vinca major',\n",
    " 'Viola palustris',\n",
    " 'Volucella',\n",
    " 'Wellingtonia',\n",
    " 'Zostera marina',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "result = {}\n",
    "for query in taxonomy:\n",
    "    print(f'Fetching: https://api.gbif.org/v1/species/suggest?q={query}')\n",
    "    response = requests.get('https://api.gbif.org/v1/species/suggest?q=' + query)\n",
    "    data = json.loads(response.text)\n",
    "    if data:\n",
    "        print(f\"Result: {data[0]}\")\n",
    "        result[query] = data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why do you think it took so long? How can you tell if no match was found?\n",
    "\n",
    "We now have all sorts of exciting information about these species names. Try some of the entity names to see if the search got the correct match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity = 'Lobelia urens'\n",
    "gbif = result[entity]\n",
    "rank = gbif['rank']\n",
    "status = gbif['status']\n",
    "gbif_id = gbif['key']\n",
    "\n",
    "print(f'Entity: {entity}, rank: {rank}, status: {status}, gbif_id: {gbif_id}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "## Named Entities and Knowledge Bases\n",
    "A **knowlege base** is a system that stores facts and in some way links them with one another into a store of information that can be queried for new knowledge. A knowledge base may store semantic information with **triples** to create a **knowledge graph** where entities (nodes) are linked to other entities (nodes) by relationships (edges).\n",
    "\n",
    "Formally, a triple is made up of subject, predicate and object. For example:\n",
    "\n",
    "> \"Odysseus\" (subject) -> \"is married to\" (predicate) -> \"Penelope\" (object)\n",
    "\n",
    "Many triples together form a graph:\n",
    "\n",
    "![Knowledge graph](https://dvcs.w3.org/hg/rdf/raw-file/default/rdf-primer/example-graph.jpg)\n",
    "\n",
    "Each entity is represented by a URI, which is unique and identifies it unambigiously.\n",
    "\n",
    "Perhaps the most well-known knowledge base is [Wikidata](https://www.wikidata.org), which is collaborative (relies on data donations and user editing) and open (all the data is openly licensed for re-use).\n",
    "\n",
    "You can get an idea of the vast store of data and query possibilities by using the [Wikidata Query Service](https://query.wikidata.org/).\n",
    "\n",
    "> **EXERCISE**: Try some of the 'Examples' queries from the [Wikidata Query Service](https://query.wikidata.org/). Notice that some queries come with visualisations. Why do you think it takes so long for some of the queries to complete?\n",
    "\n",
    "### Find Named Entities in a Knowledge Base\n",
    "To interact with Wikidata's knowledge base programmatically, we must use a W3C-standard query language called **SPARQL** (SPARQL Protocol And RDF Query Language).\n",
    "\n",
    "You can see the SPARQL queries in the Wikidata Query Service examples. They look like this:\n",
    "\n",
    "```\n",
    "#Map of hospitals\n",
    "#added 2017-08\n",
    "#defaultView:Map\n",
    "SELECT * WHERE {\n",
    "  ?item wdt:P31/wdt:P279* wd:Q16917;\n",
    "        wdt:P625 ?geo .\n",
    "}\n",
    "```\n",
    "\n",
    "Unfortunately, SPARQL has a demanding learning curve, but fortunately there are a number of [tools for programmers](https://www.wikidata.org/wiki/Wikidata:Tools/For_programmers) that can make our lives easier. \n",
    "\n",
    "We are going to use a Python package called **[wptools](https://github.com/siznax/wptools/)** to make querying Wikidata as easy as writing simple Python. wptools actually uses the [MediaWiki API](https://www.mediawiki.org/wiki/API:FAQ), which is cheating, or a good idea to avoid SPARQL, or both. ðŸ˜†\n",
    "\n",
    "First, let's try a simple string query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wptools\n",
    "\n",
    "# Construct a query for the string \"Lobelia urens\"\n",
    "page = wptools.page(\"Lobelia urens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Wikidata and show it\n",
    "page.get_wikidata()\n",
    "page.data['wikibase']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ID that is printed out `'Q3257667'` is the unique Wikidata ID, and the `wikidata_url` goes directly to the plant's unique URI. \n",
    "\n",
    "> **EXERCISE**: Try the Wikidata URL now and examine all the information that Wikidata knows about Lobelia urens. Notice in particular that it has a link to the GBIF ID '5408353'.\n",
    "\n",
    "We can even get the plant's picture programmatically!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some modules to help display images in Jupyter notebook code cells\n",
    "from IPython.display import Image\n",
    "\n",
    "# Get the picture URL from the Wikidata info\n",
    "lobelia_pic_url = page.images()[0]['url']\n",
    "\n",
    "# Display the image\n",
    "Image(url=lobelia_pic_url, embed=True, width=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **EXERCISE**: Try searching Wikidata for some of the other taxonomic names and fetching their pictures. What happens if the search is unsuccessful?\n",
    "\n",
    "Since Wikidata already has a link to the GBIF ID that we have from before, can we query Wikidata directly with the GBIF ID and get the knowledge base information that way? \n",
    "\n",
    "The answer is yes! But we will have to make a small dive into the world of SPARQL...\n",
    "\n",
    "### Make Simple SPARQL Queries\n",
    "Rather than use the Wikidata Query Service like a human, we're going to interact with the SPARQL **endpoint** programmatically. An endpoint is the URL where you send a query for a particular web service. For the curious, here is a big list of known [SPARQL endpoints](https://www.w3.org/wiki/SparqlEndpoints).\n",
    "\n",
    "Wikidata is the top entry on that list! But the endpoint listed is a bit out of date. We are going to use the [main Wikidata SPARQL endpoint](https://www.mediawiki.org/wiki/Wikidata_Query_Service/User_Manual#SPARQL_endpoint) at: https://query.wikidata.org/sparql\n",
    "\n",
    "We're going to use a different Python library called **[SPARQLWrapper](https://github.com/RDFLib/sparqlwrapper)** to make the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SPARQLWrapper import SPARQLWrapper, XML, JSON\n",
    "\n",
    "# Set the endpoint URL\n",
    "# We are a bot/script! So we also need to send a descriptive user-agent otherwise we get blocked!\n",
    "sparql = SPARQLWrapper(\"https://query.wikidata.org/sparql\", \n",
    "                       agent=\"Cambridge Digital Humanities Data School lab@cdh.cam.ac.uk\")\n",
    "\n",
    "# SPARQL query\n",
    "sparql.setQuery(\"\"\"\n",
    "SELECT * WHERE {\n",
    "  ?item wdt:P846 \"5408353\"\n",
    "}\n",
    "\"\"\")\n",
    "\n",
    "# The endpoint returns results in XML but we want to convert to JSON because it's easier to work with\n",
    "sparql.setReturnFormat(JSON)\n",
    "result = sparql.query().convert()\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you now cut and paste the URL that has been returned you should find yourself looking once again at the Lobelia entity. So far so good.\n",
    "\n",
    "Let's take a moment to understand a bit more about the SPARQL query we just made:\n",
    "\n",
    "* `SELECT *` means \"select all\" i.e. we want all the information available\n",
    "* `WHERE {}` is a clause to filter the results by whatever is between the curly braces `{}`\n",
    "* `?item wdt:P846 \"5408353\"` is a triple:\n",
    " * `?item` means \"any items (subjects) that match\"\n",
    " * `wdt:P846` is a property (predicate) and in this case the property [`P846`](https://www.wikidata.org/wiki/Property:P846) is GBIF ID\n",
    " * `\"5408353\"` is the specific ID (object) we are looking for. We got this from querying the GBIF endpoint above.\n",
    " \n",
    "So, overall, the query says \"select all information about any entity that has a GBIF ID property of 5408353\".\n",
    "\n",
    "> You can read more about [Wikidata Identifiers](https://www.wikidata.org/wiki/Wikidata:Identifiers) like \"P846\" and [Wikidata prefixes](https://www.mediawiki.org/wiki/Wikidata_Query_Service/User_Manual#Basics_-_Understanding_Prefixes) like \"wdt:\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try something a bit more sophisticated, by asking for some additional information available in Wikidata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparql.setQuery(\"\"\"\n",
    "SELECT ?item ?itemLabel ?itemDescription ?pic ?taxon ?rank WHERE {\n",
    "\n",
    "  ?item wdt:P846 \"5408353\" ;\n",
    "\n",
    "  OPTIONAL{?item wdt:P18 ?pic .}\n",
    "  OPTIONAL{?item wdt:P225 ?taxon .}\n",
    "  OPTIONAL{?item wdt:P105 ?rank .}\n",
    "  \n",
    "  SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\" }\n",
    "}\n",
    "\"\"\")\n",
    "\n",
    "result = sparql.query().convert()\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> If SPARQL takes your interest, and you'd like to learn more about linked open data, I can recommend the *Programming Historian*'s [Introduction to the Principles of Linked Open Data](https://programminghistorian.org/en/lessons/intro-to-linked-data#querying-rdf-with-sparql) and [Using SPARQL to access Linked Open Data](https://programminghistorian.org/en/lessons/retired/graph-databases-and-SPARQL) (this lesson has now been retired).\n",
    "\n",
    "Finally, let's use wptools again to get all the data we might ever want about this plant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quickly parse out the Wikidata unique ID\n",
    "url = result['results']['bindings'][0]['item']['value']\n",
    "id = url.rpartition('/')[-1]\n",
    "id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page2 = wptools.page(wikibase=id)\n",
    "page2.get_wikidata()\n",
    "page2.data['wikibase']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference this time is that we looked up the Wikidata ID first, using the unique GBIF ID, so we know we will get the info from the correct entity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "## Enrich the Original Data\n",
    "Let's take a moment to consider the journey we have travelled.\n",
    "\n",
    "<a title=\"A view of the northern ascent of Catbells (facing south) in the Lake District near Keswick, Cumbria. Diliff, CC BY-SA 3.0 &lt;https://creativecommons.org/licenses/by-sa/3.0&gt;, via Wikimedia Commons\" href=\"https://commons.wikimedia.org/wiki/File:Catbells_Northern_Ascent,_Lake_District_-_June_2009.jpg\"><img width=\"1024\" alt=\"Catbells Northern Ascent, Lake District - June 2009\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/3/38/Catbells_Northern_Ascent%2C_Lake_District_-_June_2009.jpg/1024px-Catbells_Northern_Ascent%2C_Lake_District_-_June_2009.jpg\"></a>\n",
    "\n",
    "* We started with blocks of *unstructured text* parsed from TEI XML documents.\n",
    "* We ran the text through a *machine learning model* that predicted named entities within the text.\n",
    "* We took a list of named entities and found *linked data* in various external sources of truth.\n",
    "\n",
    "We could do many things with extra information like this:\n",
    "* Add it the catalogue records for the collection.\n",
    "* Store it in a database to improve search and discovery.\n",
    "* Display it on a website along with the original documents to give extra context.\n",
    "* Create new markup with the linked data.\n",
    "\n",
    "I'm sure you can think of more ideas!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add New XML Markup for Named Entities\n",
    "To finish our exploration in code of this topic, I will show you a proof-of-concept for how we could add new TEI markup to an original Henslow Correspondence Project letter. I have had to make some simplifications in the example for the sake of brevity.\n",
    "\n",
    "<a title=\"Lobelia urens. Hans Hillewaert at the English language Wikipedia, CC BY-SA 3.0 &lt;https://creativecommons.org/licenses/by-sa/3.0/&gt;, via Wikimedia Commons\" href=\"https://commons.wikimedia.org/wiki/File:Lobelia_urens_(spike).jpg\"><img width=\"256\" alt=\"Lobelia urens (spike)\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/4/43/Lobelia_urens_%28spike%29.jpg/256px-Lobelia_urens_%28spike%29.jpg\"></a>\n",
    "<p style=\"text-align: center; font-style: italic;\">Heath lobelia close to Brigueuil, Charente, France</p>\n",
    "\n",
    "First, we will go back to the beginning of our journey and get the original letter where the binomial \"Lobelia urens\" appears. We can search the XML for the named entity and wrap it in a new XML tag to mark its position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "taxon = \"Lobelia urens\"\n",
    "\n",
    "# Get transcription from original TEI\n",
    "with open(\"data/henslow/letters_14.xml\", encoding=\"utf-8\") as file:\n",
    "    xml = file.read()\n",
    "    \n",
    "    # Find the species name and wrap it in a new XML tag\n",
    "    new_xml = xml.replace(\"Lobelia urens\", \"<name>Lobelia urens</name>\")\n",
    "    \n",
    "    # Create soup from the new XML including the new tag\n",
    "    letter = BeautifulSoup(new_xml, \"lxml-xml\")\n",
    "    \n",
    "transcription = letter.find(type='transcription')\n",
    "transcription"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you see where we have added the new tag wrapping the named entity?\n",
    "\n",
    "Now we want to modify this markup with the linked data we collected earlier, as follows:\n",
    "\n",
    "`<name type=\"taxon\" ref=\"https://www.gbif.org/species/5408353 https://www.wikidata.org/wiki/Q3257667\">Lobelia urens</name>`\n",
    "\n",
    "(Thanks to Huw Jones for supplying the correct TEI form to follow.)\n",
    "\n",
    "We can create the new markup using BeautifulSoup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data from the previous lookup steps\n",
    "gbif_url = \"https://www.gbif.org/species/5408353\"\n",
    "wikidata_url = \"https://www.wikidata.org/entity/Q3257667\"\n",
    "\n",
    "# Create the new tag and contents\n",
    "taxon_tag = letter.new_tag(\"name\", type=\"taxon\", ref=f'{gbif_url} {wikidata_url}')\n",
    "taxon_tag.string = taxon\n",
    "taxon_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then place it into the XML:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the taxonomic name in the transcription and replace it with the new tag\n",
    "transcription_tag = letter.find(type='transcription')\n",
    "transcription_tag.find(\"name\").replace_with(taxon_tag)\n",
    "\n",
    "# Print out the first paragraph of the transcription to check the new tag\n",
    "print(transcription_tag.p.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can save the new TEI document to file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "output_file = Path('output/letters_14-taxon.xml')\n",
    "letter_xml = letter.prettify()\n",
    "output_file.open('w', encoding='utf-8').write(letter_xml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **EXERCISE**: Review the modified TEI file [output/letters_14-taxon.xml](output/letters_14-taxon.xml) and inspect the newly added markup. You may need to download it and open it in Oxygen or another editor to see the markup.\n",
    "\n",
    "Of course Linked Open Data works both ways: once you have gone to the trouble of linking everything to its Wikidata ID, you may wish to [add your data to Wikidata](https://www.wikidata.org/wiki/Wikidata:Data_donation), but that is a big topic for another day.\n",
    "\n",
    "---\n",
    "---\n",
    "## Predict Entity Links with Machine Learning\n",
    "One final question may have occurred to you during the process of working through this notebook.\n",
    "\n",
    "> If linking is done automatically, potentially without human intervention, how can we be sure the results are accurate?\n",
    "\n",
    "It's likely in a real-world project you would need some form of human quality control, but an additional approach is to use machine learning to predict links.\n",
    "\n",
    "There are potentially two ways of doing this:\n",
    "\n",
    "1. Build your own **entity linker** with machine learning.\n",
    "\n",
    "spaCy has the capability to [link named entities to identifiers stored in a knowledge base](https://spacy.io/api/entitylinker/). For anyone with a lot of computing power and time to hand, there's even some [example code](https://github.com/explosion/projects/tree/master/nel-wikipedia) to do this with Wikipedia and Wikidata data dumps.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/d/d9/EDSAC_2_1960.jpg\" alt=\"EDSAC II, 10th May 1960, user queue. Copyright Computer Laboratory, University of Cambridge. Reproduced by permission. Creative Commons Attribution 2.0 UK: England & Wales.\" title=\"EDSAC II, 10th May 1960, user queue. Copyright Computer Laboratory, University of Cambridge. Reproduced by permission. Creative Commons Attribution 2.0 UK: England & Wales.\">\n",
    "<p style=\"text-align: center; font-style: italic;\">The queue for computing time on the Cambridge EDSAC, 1960. To use High Performance Computing today, nothing has really changed, except the queue itself is now managed by software!</p>\n",
    "\n",
    "2. Use someone else's pre-built entity linker if they have built something suitable for your use case.\n",
    "\n",
    "You can check [spaCy Universe](https://spacy.io/universe) for resources developed with or for spaCy. One example of an entity linker:\n",
    "\n",
    "* [Mordecai](https://github.com/openeventdata/mordecai) uses spaCy to extract place names, predict which [GeoNames](https://www.geonames.org/) entity is the best match, and return the linked geographic information.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/openeventdata/mordecai/master/paper/mordecai_geoparsing.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "## Summary\n",
    "\n",
    "Covered in this notebook:\n",
    "\n",
    "* **Linked data** can enrich the discovery of collections and allow sophisticated searches for the knowledge within those collections.\n",
    "* **Linked open data** is linked data that is openly licensed for re-use.\n",
    "* We can disambiguate a named entity by linking it to an **authority file** or other source of truth with a **unique identifier**.\n",
    "* Authorities and open data aggregators can be accessed via **web APIs**, which allow rapid and automated query and retrieval of information.\n",
    "* **Wikidata** is an openly licensed **knowledge base** containing a **knowledge graph** of **triples**. It has several web APIs that you can query with the language **SPARQL**.\n",
    "* After linking named entities to external identifiers it's possible to **enrich** the original data and, in return, donate data to knowledge bases like Wikidata.\n",
    "* **Machine learning** can also be used when linking named entities automatically, to improve the likelihood of linking to the correct entity.\n",
    "\n",
    "Congratulations! ðŸŽ‰ \n",
    "\n",
    "That's the end of this series of notebooks about named entity recognition. I hope you enjoyed your time working through them."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
